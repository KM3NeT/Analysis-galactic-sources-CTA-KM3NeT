{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the KM3NeT datasets\n",
    "The script produces pseudo datasets from KM3NeT IRFs. As production takes quite long, you can either reduce the time binning of the data generated below or download the results externally - see how in the data/km3net - folder.\n",
    "\n",
    "To plot the results of this data set generation, see the `Plot_KM3NeT_datasets.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import EarthLocation, SkyCoord, AltAz\n",
    "from astropy.time import Time\n",
    "\n",
    "from gammapy.data import Observation\n",
    "from gammapy.datasets import MapDataset, Datasets\n",
    "from gammapy.maps import WcsGeom, MapAxis, WcsNDMap\n",
    "from gammapy.makers import MapDatasetMaker\n",
    "from gammapy.irf import PSF3D, EnergyDispersion2D, Background2D, EffectiveAreaTable2D\n",
    "from gammapy.modeling.models import BackgroundModel\n",
    "\n",
    "from regions import CircleSkyRegion\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from flux_utils import SourceModel\n",
    "import plot_utils\n",
    "\n",
    "from configure_analysis import AnalysisConfig\n",
    "analysisconfig = AnalysisConfig()\n",
    "\n",
    "%matplotlib inline\n",
    "plot_utils.mpl_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on source VelaX at position <SkyCoord (ICRS): (ra, dec) in deg\n",
      "    (128.287, -45.19)>\n"
     ]
    }
   ],
   "source": [
    "# The source can be set in the analysis_config.yml file\n",
    "source_name = analysisconfig.get_source()\n",
    "model = SourceModel(source_name)\n",
    "print(\"Working on source\", source_name, \"at position\", model.position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visibility\n",
    "\n",
    "First, the zenith angles under which the source can be observed from the KM3NeT (ARCA) position needs to be calculated.\n",
    "\n",
    "In the analysis, the time_step is set such that an hourly binning is used for generation of the datasets. \n",
    "Here, for easier execution, a daily binning is set (parameter `hours = 24` in `analysis_config.yml`). Change `analysis_config.yml` if it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCA position: (4943908.93847172, 1426985.99079506, 3750019.30219182) m\n",
      "Time steps 28800 with step duration 8\n"
     ]
    }
   ],
   "source": [
    "pos_arca = EarthLocation.from_geodetic(\n",
    "    lat=analysisconfig.get_value(\"latitude\", \"km3net_datasets.detector_position\"), \n",
    "    lon=analysisconfig.get_value(\"longitude\", \"km3net_datasets.detector_position\"), \n",
    "    height=analysisconfig.get_value(\"height\", \"km3net_datasets.detector_position\")\n",
    ")\n",
    "\n",
    "print (\"ARCA position:\", pos_arca)\n",
    "\n",
    "time_step = analysisconfig.get_value(\"hours\", \"km3net_datasets\")*3600 # in seconds\n",
    "start = Time('2019-01-01T00:00:00', format='isot').unix\n",
    "end = Time('2020-01-01T00:00:00', format='isot').unix\n",
    "times = np.linspace(start, end-time_step, int(365*24*3600/time_step))\n",
    "obstimes = Time(times, format='unix')\n",
    "\n",
    "frames = AltAz(obstime=obstimes, location=pos_arca)\n",
    "print (\"Time steps\", time_step, \"with step duration\", analysisconfig.get_value(\"hours\", \"km3net_datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform source position into local coordinates for each time step\n",
    "local_coords = model.position.transform_to(frames)\n",
    "\n",
    "# look at the minimum and maximum zenith angle (0 = above, 180 = below)\n",
    "zen_angles = local_coords.zen.value\n",
    "\n",
    "# zenith angle binning\n",
    "cos_zen_bins = np.linspace(-1, 1, analysisconfig.get_value(\"zenithbinning\", \"km3net_datasets\"))\n",
    "cos_zen_binc = 0.5 * (cos_zen_bins[:-1] + cos_zen_bins[1:])\n",
    "zen_bins = np.arccos(cos_zen_bins) * 180 / np.pi\n",
    "zen_binc = np.arccos(cos_zen_binc) * 180 / np.pi\n",
    "\n",
    "# compute visibility time for each zenith angle bin\n",
    "vis_hist = np.histogram(np.cos(zen_angles*np.pi/180), bins=cos_zen_bins)\n",
    "vis_times = vis_hist[0] / vis_hist[0].sum() * u.yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160.63116486887048\n",
      "139.1090246529909\n",
      "125.90772606852988\n",
      "114.66403651518051\n",
      "104.40592321688483\n",
      "94.59478747222855\n",
      "84.4333213533851\n"
     ]
    }
   ],
   "source": [
    "# determine which zenith angle bins are going to be used\n",
    "bin_mask = (zen_binc > 80) & (vis_hist[0] > 0)\n",
    "\n",
    "# how many datasets will be used\n",
    "n_datasets = bin_mask.sum()\n",
    "\n",
    "# compute dataset indices for all time bins\n",
    "dataset_idx = np.digitize(zen_angles, bins=zen_bins)\n",
    "# for what it was done?!\n",
    "dataset_idx -= dataset_idx.min()\n",
    "\n",
    "# compute masks for each zenith angle bin\n",
    "zen_masks = []\n",
    "for i in range(n_datasets):\n",
    "    z1 = zen_bins[1:][bin_mask][i]\n",
    "    z2 = zen_bins[:-1][bin_mask][i]\n",
    "    zen_masks.append((zen_angles > z1) & (zen_angles < z2))\n",
    "    \n",
    "# compute average zenith angle for each zenith angle bin\n",
    "zen_mean = [zen_angles[m].mean() for m in zen_masks]\n",
    "for i in zen_mean: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Datasets\n",
    "\n",
    "### Setting of the geometry\n",
    "\n",
    "for each of the zenith angle bins a dataset will be created with the corresponding live time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up geometries\n",
    "\n",
    "# set the true and reconstructed energy axes\n",
    "energy_axis = MapAxis.from_bounds(1e2, 1e6, nbin=16, unit='GeV', name='energy', interp='log')\n",
    "energy_axis_true = MapAxis.from_bounds(1e2, 1e7, nbin=80, unit='GeV', name='energy_true', interp='log')\n",
    "\n",
    "# also set the energy migration axis for the energy dispersion and the theta axis for the PSF\n",
    "migra_axis = MapAxis.from_edges(np.logspace(-5, 2, 57), name='migra')\n",
    "theta_axis = MapAxis.from_edges(np.linspace(0, 8, 101), name='theta', unit='deg')\n",
    "\n",
    "# create the geometries\n",
    "geom = WcsGeom.create(\n",
    "    binsz  = 0.1*u.deg,\n",
    "    width  = 30*u.deg,\n",
    "    skydir = model.position,\n",
    "    frame  = 'icrs',\n",
    "    axes   = [energy_axis],\n",
    ")\n",
    "geom_true = WcsGeom.create(\n",
    "    binsz  = 0.1*u.deg,\n",
    "    width  = 30*u.deg,\n",
    "    skydir = model.position,\n",
    "    frame  = 'icrs',\n",
    "    axes=[energy_axis_true],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IACT arrays have a pointing position, so we need to choose one here as well. Usually, the pointing position is used to compute an offset angle, which is then used to evaluate the IRFs. Below, we define some patches that make sure we evaluate the background model and the effective area at the correct zenith angle for each spatial pixel and time bin. The PSF and EDISP (which vary less strongly) are still evaluated based on the angle between the pointing we define here and the source position. Therefore, we simply rotate the source position along the declination axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetime_pointings = vis_times[bin_mask] * 10  # 10 years\n",
    "\n",
    "pointings = model.position.directional_offset_by(0*u.deg, np.array(zen_mean)*u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.4 s, sys: 2.04 s, total: 39.5 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "# transform all map coordinates into local coordinate system\n",
    "map_coord = geom_true.to_image().get_coord()\n",
    "sky_coord = map_coord.skycoord\n",
    "map_coord_zeniths = [sky_coord.transform_to(frames[i]).zen.value for i in range(len(obstimes))]\n",
    "map_coord_zeniths = np.array(map_coord_zeniths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read KM3NeT IRF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the IRFs\n",
    "aeff = EffectiveAreaTable2D.read(analysisconfig.get_file(\"km3net/irfs/aeff.fits\"))\n",
    "edisp = EnergyDispersion2D.read(analysisconfig.get_file(\"km3net/irfs/edisp.fits\"))\n",
    "psf = PSF3D.read(analysisconfig.get_file(\"km3net/irfs/psf.fits\"))\n",
    "\n",
    "# Also the neutrino / muon background\n",
    "bkg_nu = Background2D.read(analysisconfig.get_file(\"km3net/irfs/bkg_nu.fits\"))\n",
    "bkg_mu = Background2D.read(analysisconfig.get_file(\"km3net/irfs/bkg_mu.fits\"))\n",
    "\n",
    "irfs = dict(aeff=aeff, psf=psf, edisp=edisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create observations\n",
    "obs_list = [Observation.create(pointing=pointings[i], livetime=livetime_pointings[i], irfs=irfs)\\\n",
    "            for i in range(n_datasets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 2.08 s, total: 14 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "# Make the MapDatasets\n",
    "empty = MapDataset.create(\n",
    "    geom = geom, \n",
    "    energy_axis_true = energy_axis_true, \n",
    "    migra_axis = migra_axis, \n",
    "    rad_axis = theta_axis, \n",
    "    binsz_irf = 1\n",
    ")\n",
    "\n",
    "dataset_maker = MapDatasetMaker(selection=['exposure', 'psf', 'edisp'])\n",
    "\n",
    "datasets = []\n",
    "for i,obs in enumerate(obs_list):\n",
    "    dataset = dataset_maker.run(empty.copy(name='nu{}'.format(i+1)), obs)\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining the pixel coordinates of the edges:\n",
    "    \n",
    "(top left, top center, top right, center left, src_pos, center right, bottom left, bottom center, bottom right)\n",
    "of the considered field-of-view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_coord_pixel_pos = [(299,0), (299,150), (299,299),\n",
    "                        (150,0), (150,150), (150,299),\n",
    "                        (  0,0), (  0,150), (  0,299)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to monkey-patch Background2D.evaluate, so that it evaluates the background at a certain zenith angle (instead of computing an offset angle). We simply pass the zenith angle as **fov_lon**.\n",
    "\n",
    "Similarly, we need a patched method to evaluate the exposure correctly.\n",
    "\n",
    "This method does the same as Gammapy's `make_map_exposure_true_energy`, except that it takes an offset angle (or, in our case, the zenith angle) instead of a pointing position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkg_2d_eval_patched(self, fov_lon, fov_lat, energy_reco, method='linear', **kwargs):\n",
    "    return self.data.evaluate(offset=fov_lon, energy=energy_reco, method='linear', **kwargs)\n",
    "\n",
    "Background2D.evaluate = bkg_2d_eval_patched\n",
    "\n",
    "# First, define a small helper function\n",
    "def calc_exposure(offset, aeff, geom):\n",
    "    energy = geom.get_axis_by_name('energy_true').center\n",
    "    exposure = aeff.data.evaluate(offset=offset, energy_true=energy[:, np.newaxis, np.newaxis])\n",
    "    return exposure\n",
    "\n",
    "def make_map_exposure_true_energy_patched(offset, livetime, aeff, geom):\n",
    "    exposure = calc_exposure(offset, aeff, geom)\n",
    "    exposure = (exposure * livetime).to('m2 s')\n",
    "    return WcsNDMap(geom, exposure.value.reshape(geom.data_shape), unit=exposure.unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute background for all edge pixels and mean zenith angle\n",
    "Below we compute the predicted background and the exposure for all edge pixels and all time bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nu_background_edge_pixels = []\n",
    "mu_background_edge_pixels = []\n",
    "exposure_edge_pixels = []\n",
    "\n",
    "d_omega = geom_true.to_image().solid_angle()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for pp in edge_coord_pixel_pos:\n",
    "        zen_vals = map_coord_zeniths[:,pp[0],pp[1]]\n",
    "    \n",
    "        # compute neutrino background for every zenith angle value\n",
    "        bkg_nu_de = bkg_nu.evaluate_integrate(\n",
    "            fov_lon = zen_vals*u.deg,\n",
    "            fov_lat = None, # does not matter\n",
    "            energy_reco = geom.get_axis_by_name('energy').edges[:, np.newaxis]\n",
    "        )\n",
    "        bkg_nu_int = (bkg_nu_de * d_omega[pp[0],pp[1]]).to_value('s-1').sum(axis=0)\n",
    "        nu_background_edge_pixels.append(bkg_nu_int)\n",
    "    \n",
    "        # compute muon background for every zenith angle value\n",
    "        bkg_mu_de = bkg_mu.evaluate_integrate(\n",
    "            fov_lon = zen_vals*u.deg,\n",
    "            fov_lat = None, # does not matter\n",
    "            energy_reco = geom.get_axis_by_name('energy').edges[:, np.newaxis]\n",
    "        )\n",
    "        bkg_mu_int = (bkg_mu_de * d_omega[pp[0],pp[1]]).to_value('s-1').sum(axis=0)\n",
    "        mu_background_edge_pixels.append(bkg_mu_int)\n",
    "    \n",
    "        # compute exposure for every zenith angle value\n",
    "        # (sum over all energies - just for this test)\n",
    "        exp = (calc_exposure(zen_vals*u.deg, aeff, geom_true).sum(axis=(0,1)) * time_step * u.s).to_value('m2 s')\n",
    "        exposure_edge_pixels.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_background_edge_pixels_zen_mean = []\n",
    "mu_background_edge_pixels_zen_mean = []\n",
    "exposure_edge_pixels_zen_mean = []\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    nu_background_edge_pixels_zen_mean.append([])\n",
    "    mu_background_edge_pixels_zen_mean.append([])\n",
    "    exposure_edge_pixels_zen_mean.append([])\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for pp in edge_coord_pixel_pos:\n",
    "            zen_vals = map_coord_zeniths[:,pp[0],pp[1]][zen_masks[i]]\n",
    "    \n",
    "            # neutrino background\n",
    "            bkg_nu_de = bkg_nu.evaluate_integrate(\n",
    "                fov_lon = zen_vals.mean()*u.deg,\n",
    "                fov_lat = None, # does not matter\n",
    "                energy_reco = geom.get_axis_by_name('energy').edges[:, np.newaxis]\n",
    "            )\n",
    "            bkg_nu_int = (bkg_nu_de * d_omega[pp[0],pp[1]]).to_value('s-1').sum(axis=0)\n",
    "            nu_background_edge_pixels_zen_mean[-1].append(bkg_nu_int[0])\n",
    "    \n",
    "            # muon background\n",
    "            bkg_mu_de = bkg_mu.evaluate_integrate(\n",
    "                fov_lon = zen_vals.mean()*u.deg,\n",
    "                fov_lat = None, # does not matter\n",
    "                energy_reco = geom.get_axis_by_name('energy').edges[:, np.newaxis]\n",
    "            )\n",
    "            bkg_mu_int = (bkg_mu_de * d_omega[pp[0],pp[1]]).to_value('s-1').sum(axis=0)\n",
    "            mu_background_edge_pixels_zen_mean[-1].append(bkg_mu_int[0])\n",
    "    \n",
    "            # compute exposure for every zenith angle value\n",
    "            # (sum over all energies - just for this test)\n",
    "            exp = (calc_exposure(zen_vals.mean()*u.deg, aeff, geom_true).sum(axis=(0,1)) * time_step * u.s).to_value('m2 s')\n",
    "            exposure_edge_pixels_zen_mean[-1].append(exp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the plots above, it seems a good idea to compute the background rate and exposure separately for each pixel and time bin. Storing this in memory is not possible, so we immediately add up the background rates / exposure for each zenith angle bin.\n",
    "\n",
    "!!! This takes O(day) for the original setup, or ~1h for a daily scan, depending on computing speed !!!\n",
    "\n",
    "You can also download the data sets from our data server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generation: 100%|████████████████████████████████| 1095/1095 [14:26<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 27s, sys: 1min 56s, total: 14min 24s\n",
      "Wall time: 14min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nu_background_maps = np.zeros((n_datasets, *geom.data_shape))\n",
    "mu_background_maps = np.zeros((n_datasets, *geom.data_shape))\n",
    "exposure_maps = np.zeros((n_datasets, *geom_true.data_shape))\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for i in tqdm(range(len(obstimes)), \"generation\"):  \n",
    "        # skip this time bin if the zenith angle of the source position is below 80 deg\n",
    "        if dataset_idx[i] >= n_datasets:\n",
    "            assert zen_angles[i] <= zen_bins[7]\n",
    "            continue\n",
    "    \n",
    "        # zenith angle map for this time bin\n",
    "        zen_vals = map_coord_zeniths[i]\n",
    "    \n",
    "        # compute neutrino background map for this time bin\n",
    "        bkg_nu_de = bkg_nu.evaluate_integrate(\n",
    "            fov_lon = zen_vals*u.deg,\n",
    "            fov_lat = None, # does not matter\n",
    "            energy_reco = geom.get_axis_by_name('energy').edges[:, np.newaxis, np.newaxis]\n",
    "        )\n",
    "        bkg_nu_de = (bkg_nu_de * d_omega).to_value('s-1')\n",
    "    \n",
    "        # add neutrino background rate for the correct dataset\n",
    "        nu_background_maps[dataset_idx[i]] += bkg_nu_de\n",
    "    \n",
    "        # compute muon background map for this time bin\n",
    "        bkg_mu_de = bkg_mu.evaluate_integrate(\n",
    "            fov_lon = zen_vals*u.deg,\n",
    "            fov_lat = None, # does not matter\n",
    "            energy_reco = geom.get_axis_by_name('energy').edges[:, np.newaxis, np.newaxis]\n",
    "        )\n",
    "        bkg_mu_de = (bkg_mu_de * d_omega).to_value('s-1')\n",
    "    \n",
    "        # add muon background rate for the correct dataset\n",
    "        mu_background_maps[dataset_idx[i]] += bkg_mu_de\n",
    "    \n",
    "        # compute exposure for this time bin\n",
    "        exp = (calc_exposure(zen_vals*u.deg, aeff, geom_true) * time_step * u.s).to_value('m2 s')\n",
    "    \n",
    "        # add exposure for the correct dataset\n",
    "        exposure_maps[dataset_idx[i]] += exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply exposure maps by 10, for 10 years of data\n",
    "exposure_maps *= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the exposure attribute for each dataset\n",
    "for i in range(n_datasets):\n",
    "    exp_map = WcsNDMap(geom_true.copy(), data=exposure_maps[i], unit='m2 s')\n",
    "    datasets[i].exposure = exp_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide the summed-up background rates for each dataset by the number of time bins that contribute to this dataset $\\rightarrow$ obtain an averaged rate, \n",
    "then multiply with the total livetime for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_datasets):\n",
    "    n_times = (dataset_idx == i).sum()\n",
    "    nu_background_maps[i] /= n_times\n",
    "    mu_background_maps[i] /= n_times\n",
    "\n",
    "    nu_background_maps[i] *= livetime_pointings[i].to_value('s')\n",
    "    mu_background_maps[i] *= livetime_pointings[i].to_value('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the atmospheric neutrino background model\n",
    "bkg_models = []\n",
    "for i in range(n_datasets):\n",
    "    # create map\n",
    "    bkg_model_map = WcsNDMap(geom.copy(), data=nu_background_maps[i])\n",
    "\n",
    "    # generate BackgroundModel instance\n",
    "    bkg_model = BackgroundModel(bkg_model_map, name=datasets[i].name + '-bkg', datasets_names=[datasets[i].name])\n",
    "\n",
    "    # store model\n",
    "    bkg_models.append(bkg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the atmospheric muon background\n",
    "bkg_models_mu = []\n",
    "for i in range(n_datasets):\n",
    "    # Create map\n",
    "    bkg_model_map = WcsNDMap(geom.copy(), data=mu_background_maps[i])\n",
    "\n",
    "    # generate BackgroundModel instance\n",
    "    bkg_model = BackgroundModel(bkg_model_map, name=datasets[i].name + 'bkg-mu', datasets_names=[datasets[i].name])\n",
    "\n",
    "    # store model\n",
    "    bkg_models_mu.append(bkg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attach only neutrino background model to datasets, to compute npred\n",
    "npred_bkg_nu = []\n",
    "for i in range(n_datasets):\n",
    "    datasets[i].models = bkg_models[i]\n",
    "    npred_bkg_nu.append(datasets[i].npred())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add the muon background (store npred for both the sum and the muon background only)\n",
    "npred_bkg_mu = []\n",
    "npred_bkg_sum = []\n",
    "for i in range(n_datasets):\n",
    "    datasets[i].background_model.stack(bkg_models_mu[i])\n",
    "    bkg_sum = datasets[i].npred()\n",
    "    bkg_mu = bkg_sum - npred_bkg_nu[i]\n",
    "    npred_bkg_mu.append(bkg_mu)\n",
    "    npred_bkg_sum.append(bkg_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the datasets to disk\n",
    "if analysisconfig.get_value(\"write_KM3NeT_pseudodata\", \"io\"):\n",
    "    ext = analysisconfig.get_value(\"km3net_pseudodata_extension\", \"io\")  # folder name extension when using different data sets\n",
    "    ds_dir = analysisconfig.get_file(\"km3net/pseudodata/KM3NeT_\"+source_name+ext)\n",
    "    os.makedirs(ds_dir, exist_ok=True)\n",
    "    Datasets(datasets).write(str(ds_dir) + \"/KM3NeT_\"+source_name+ext,\n",
    "                             str(ds_dir) + \"/KM3NeT_\"+source_name+ext, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also store npred maps for nu and mu background\n",
    "if analysisconfig.get_value(\"write_KM3NeT_pseudodata\", \"io\"):\n",
    "    bkg_dir = analysisconfig.get_file(str(ds_dir)+\"/npred_bkg_km3net\")\n",
    "    os.makedirs(bkg_dir, exist_ok=True)\n",
    "    for i in range(n_datasets):\n",
    "        nuname = str(bkg_dir) + \"/{}_npred_nu_{:02d}.fits\".format(source_name, i+1)\n",
    "        muname = str(bkg_dir) + \"/{}_npred_mu_{:02d}.fits\".format(source_name, i+1)\n",
    "        npred_bkg_nu[i].write(nuname, overwrite=True)\n",
    "        npred_bkg_mu[i].write(muname, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "km3net_cta",
   "language": "python",
   "name": "km3net_cta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
